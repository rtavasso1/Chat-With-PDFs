{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import glob\n",
    "import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pickle\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "\n",
    "filenames = glob.glob(\"example-docs/*.pdf\")\n",
    "loaders = [PyPDFLoader(filename) for filename in filenames]\n",
    "listOfPages = [loader.load_and_split() for loader in loaders] # list of list of dict with keys \"page_content\", \"metadata\" {\"source\", \"page\"}\n",
    "\n",
    "faiss_indices = {filename: FAISS.from_documents(pages, OpenAIEmbeddings(disallowed_special=())) for filename, pages in zip(filenames,listOfPages)} # dict of FAISS indexes \n",
    "\n",
    "if False: # write to file\n",
    "    with open('faiss_indices.pkl', 'wb') as f:\n",
    "        pickle.dump(faiss_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: IMAGE BIND: One Embedding Space To Bind Them All\n",
      "Rohit Girdhar∗Alaaeldin El-Nouby∗Zhuang Liu Mannat Singh\n",
      "Kalyan Vasudev Alwala Armand Joulin Ishan Misra∗\n",
      "FAIR, Meta AI\n",
      "https://facebookresearch.github.io/ImageBind\n",
      "1) Cross-Modal Retrieval\n",
      "Crackle of a Fire\n",
      "AudioText\n",
      "“A fire crackles while a pan of f \n",
      "\n",
      "1: modalities with linguistic inputs. Various methods adapt\n",
      "CLIP to extract semantically strong video representations\n",
      "[14, 42, 44, 77]. Most related to our method, Nagrani et\n",
      "al. [50] create a weakly-labeled dataset for paired video-\n",
      "audio and captions that allows for training multi-modal\n",
      "video-audio e \n",
      "\n",
      "12: Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "h \n",
      "\n",
      "13: Input-Input Layer5\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Input-Input Layer5\n",
      "The\n",
      "Law\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example search functionality\n",
    "search_docs = [\"example-docs\\\\imagebind.pdf\", \"example-docs\\\\Attention is All You Need.pdf\"]\n",
    "docs = []\n",
    "for filename in search_docs:\n",
    "    docs.extend(faiss_indices[filename].similarity_search(\"How are audio and image combined?\", k=2))\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='IMAGE BIND: One Embedding Space To Bind Them All\\nRohit Girdhar∗Alaaeldin El-Nouby∗Zhuang Liu Mannat Singh\\nKalyan Vasudev Alwala Armand Joulin Ishan Misra∗\\nFAIR, Meta AI\\nhttps://facebookresearch.github.io/ImageBind\\n1) Cross-Modal Retrieval\\nCrackle of a Fire\\nAudioText\\n“A fire crackles while a pan of food is frying on the fire.”“Fire is crackling then wind starts blowing.”“Firewood crackles then music...”\\n“A baby is crying while a toddler is laughing.”“A baby is laughing while an adult is laughing.”“A baby laughs and something…”\\nBaby Cooing\\nWaves\\n2) Embedding-Space Arithmetic3) Audio to Image GenerationDogEngineFireImagesVideosDepth&\\nRain\\nFigure 1. I MAGE BIND’s joint embedding space enables novel multimodal capabilities. By aligning six modalities’ embedding into a\\ncommon space, I MAGE BINDenables: 1)Cross-Modal Retrieval, which shows emergent alignment of modalities such as audio, depth or\\ntext, that aren’t observed together. 2)Adding embeddings from different modalities naturally composes their semantics. And 3)Audio-to-\\nImage generation, by using our audio embeddings with a pre-trained DALLE-2 [60] decoder designed to work with CLIP text embeddings.\\nAbstract\\nWe present IMAGE BIND, an approach to learn a joint\\nembedding across six different modalities - images, text, au-\\ndio, depth, thermal, and IMU data. We show that all combi-\\nnations of paired data are not necessary to train such a joint\\nembedding, and only image-paired data is sufficient to bind\\nthe modalities together. IMAGE BIND can leverage recent\\nlarge scale vision-language models, and extends their zero-\\nshot capabilities to new modalities just by using their natu-\\nral pairing with images. It enables novel emergent applica-\\ntions ‘out-of-the-box’ including cross-modal retrieval, com-\\nposing modalities with arithmetic, cross-modal detection\\nand generation. The emergent capabilities improve with the\\nstrength of the image encoder and we set a new state-of-the-\\nart on emergent zero-shot recognition tasks across modal-\\nities, outperforming specialist supervised models. Finally,\\nwe show strong few-shot recognition results outperforming\\nprior work, and that IMAGE BINDserves as a new way to\\nevaluate vision models for visual and non-visual tasks.\\n∗Equal technical contribution.1. Introduction\\nA single image can bind together many experiences – an\\nimage of a beach can remind us of the sound of waves, the\\ntexture of the sand, a breeze, or even inspire a poem. This\\n‘binding’ property of images offers many sources of super-\\nvision to learn visual features, by aligning them with any\\nof the sensory experiences associated with images. Ideally,\\nfor a single joint embedding space, visual features should\\nbe learned by aligning to all of these sensors. However, this\\nrequires acquiring all types and combinations of paired data\\nwith the same set of images, which is infeasible.\\nRecently, many methods learn image features aligned\\nwith text [1, 30, 45, 59, 63, 80, 81], audio [3, 4, 49, 54,\\n55, 68] etc. These methods use a single pair of modali-\\nties or, at best, a few visual modalities. However, the fi-\\nnal embeddings are limited to the pairs of modalities used\\nfor training. Thus, video-audio embeddings cannot directly\\nbe used for image-text tasks and vice versa. A major ob-\\nstacle in learning a true joint embedding is the absence of\\nlarge quantities of multimodal data where all modalities are\\npresent together.arXiv:2305.05665v1  [cs.CV]  9 May 2023', metadata={'source': 'example-docs\\\\imagebind.pdf', 'page': 0}),\n",
       " Document(page_content='modalities with linguistic inputs. Various methods adapt\\nCLIP to extract semantically strong video representations\\n[14, 42, 44, 77]. Most related to our method, Nagrani et\\nal. [50] create a weakly-labeled dataset for paired video-\\naudio and captions that allows for training multi-modal\\nvideo-audio encoder to match textual features resulting in\\nstrong audio and video retrieval and captioning perfor-\\nmance. AudioCLIP [26] adds audio as an additional modal-\\nity into a CLIP framework, enabling zero-shot audio classi-\\nfication. In contrast, I MAGE BINDdoes not require explicit\\npaired data between all modalities and instead leverages im-\\nage as a natural weak supervision for unifying modalities.\\nFeature Alignment Pre-trained CLIP models have been\\nutilized as teachers to supervise other models due to the\\nstrength of its visual representations [43, 57, 73]. More-\\nover, CLIP joint image and text embedding space has also\\nbeen leveraged for a variety of zero-shot tasks like de-\\ntection [23, 86], segmentation [40], mesh animation [79]\\netc. showing the power of joint embedding spaces. Point-\\nCLIP [83] finds a pre-trained CLIP encoder can be used for\\n3D recognition by projecting a point cloud to a number of\\n2D depth map views, which in turn are encoded using CLIP\\nvisual encoder. In multilingual neural machine translation,\\na similar phenomenon to the emergence behavior of I M-\\nAGEBINDis commonly observed and utilized: if languages\\nare trained in the same latent space through learned implicit\\nbridging, translation can be done between language pairs on\\nwhich no paired data is provided [32, 39].\\n3. Method\\nOur goal is to learn a single joint embedding space for all\\nmodalities by using images to bind them together. We align\\neach modality’s embedding to image embeddings, such as\\ntext to image using web data and IMU to video using video\\ndata captured from egocentric cameras with IMU. We show\\nthat the resulting embedding space has a powerful emer-\\ngent zero-shot behavior that automatically associates pairs\\nof modalities without seeing any training data for that spe-', metadata={'source': 'example-docs\\\\imagebind.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weiner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
